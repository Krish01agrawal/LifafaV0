#!/usr/bin/env python3
"""
Scalable Email Processor for 10,000+ Emails
===========================================

This script leverages the existing intelligent batch processing architecture
to handle large volumes of emails efficiently and cost-effectively.

Key Features:
1. Intelligent batch categorization (75 emails per batch, 3 concurrent)
2. Advanced financial extraction with validation
3. MongoDB optimization and indexing
4. Progress tracking and monitoring
5. Error handling and recovery
6. Cost optimization (90% reduction)

Architecture:
- Stage 1: Email categorization using IntelligentBatchCategorizer
- Stage 2: Financial extraction using AdvancedFinancialExtractor  
- Stage 3: MongoDB optimization and indexing
- Stage 4: Query optimization and caching
"""

import asyncio
import logging
from datetime import datetime
from typing import Dict, Any, Optional, List
from bson import ObjectId

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import the intelligent processing systems
from app.intelligent_batch_categorizer import (
    start_batch_categorization,
    get_categorization_status,
    BatchCategorizationResult,
    IntelligentBatchCategorizer
)
from app.advanced_financial_extractor import (
    start_financial_extraction,
    get_financial_extraction_status,
    AdvancedFinancialExtractor
)
from app.mongodb_optimizer import (
    optimize_database_indexes,
    get_query_performance_stats
)
from app.services.database_service import DatabaseService
from app.workers.email_worker import EmailWorker

class ScalableEmailProcessor:
    """Scalable email processor for large volumes"""
    
    def __init__(self):
        self.batch_categorizer = IntelligentBatchCategorizer()
        self.financial_extractor = AdvancedFinancialExtractor()
        self.email_worker = EmailWorker()
        
    async def process_user_emails_scalable(
        self, 
        user_id: str, 
        email_limit: Optional[int] = None,
        force_reprocess: bool = False
    ) -> Dict[str, Any]:
        """
        Process all emails for a user using scalable batch processing
        
        This is the main entry point that orchestrates the complete pipeline:
        1. Batch categorization (75 emails per batch, 3 concurrent)
        2. Financial extraction with validation
        3. MongoDB optimization
        4. Progress tracking
        """
        try:
            start_time = datetime.now()
            logger.info(f"ğŸš€ Starting scalable email processing for user {user_id}")
            
            # Initialize database
            await DatabaseService.initialize()
            db = DatabaseService.get_database()
            
            # Get total email count
            total_emails = await db.email_logs.count_documents({"user_id": user_id})
            if email_limit:
                total_emails = min(total_emails, email_limit)
            
            logger.info(f"ğŸ“Š Total emails to process: {total_emails}")
            
            if total_emails == 0:
                return {
                    "success": True,
                    "message": "No emails found to process",
                    "user_id": user_id,
                    "total_emails": 0,
                    "processing_time": 0
                }
            
            # Stage 1: Intelligent Batch Categorization
            logger.info("ğŸ“‚ Stage 1: Starting intelligent batch categorization")
            categorization_result = await self._run_batch_categorization(user_id, email_limit, force_reprocess)
            
            if not categorization_result.success:
                return {
                    "success": False,
                    "error": "Batch categorization failed",
                    "user_id": user_id,
                    "categorization_result": categorization_result
                }
            
            # Stage 2: Financial Data Extraction
            logger.info("ğŸ’° Stage 2: Starting financial data extraction")
            financial_result = await self._run_financial_extraction(user_id, email_limit)
            
            # Stage 3: MongoDB Optimization
            logger.info("âš¡ Stage 3: Optimizing MongoDB performance")
            optimization_result = await self._optimize_database()
            
            # Stage 4: Final Status Check
            logger.info("ğŸ“Š Stage 4: Generating final status report")
            final_status = await self._generate_final_status(user_id)
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            logger.info(f"ğŸ‰ Scalable email processing completed for user {user_id}")
            logger.info(f"â±ï¸ Total processing time: {processing_time:.2f} seconds")
            
            return {
                "success": True,
                "user_id": user_id,
                "total_emails": total_emails,
                "processing_time": processing_time,
                "categorization_result": categorization_result,
                "financial_result": financial_result,
                "optimization_result": optimization_result,
                "final_status": final_status,
                "completed_at": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ Error in scalable email processing: {e}")
            return {
                "success": False,
                "error": str(e),
                "user_id": user_id
            }
    
    async def _run_batch_categorization(
        self, 
        user_id: str, 
        email_limit: Optional[int] = None,
        force_reprocess: bool = False
    ) -> BatchCategorizationResult:
        """Run intelligent batch categorization"""
        try:
            logger.info(f"ğŸ“‚ Running batch categorization for user {user_id}")
            
            # Use the intelligent batch categorizer
            result = await start_batch_categorization(
                user_id=user_id,
                limit=email_limit,
                force_reprocess=force_reprocess
            )
            
            logger.info(f"âœ… Batch categorization completed:")
            logger.info(f"   ğŸ“Š Total emails: {result.total_emails}")
            logger.info(f"   âœ… Processed: {result.processed_emails}")
            logger.info(f"   ğŸ“‚ Categorized: {result.categorized_emails}")
            logger.info(f"   â±ï¸ Processing time: {result.processing_time:.2f}s")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Error in batch categorization: {e}")
            raise
    
    async def _run_financial_extraction(self, user_id: str, email_limit: Optional[int] = None) -> Dict[str, Any]:
        """Run financial data extraction"""
        try:
            logger.info(f"ğŸ’° Running financial extraction for user {user_id}")
            
            # Use the advanced financial extractor
            result = await start_financial_extraction(
                user_id=user_id,
                limit=email_limit
            )
            
            logger.info(f"âœ… Financial extraction completed:")
            logger.info(f"   ğŸ’³ Extracted transactions: {result.get('extracted_transactions', 0)}")
            logger.info(f"   ğŸ“Š Processing time: {result.get('processing_time', 0):.2f}s")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Error in financial extraction: {e}")
            raise
    
    async def _optimize_database(self) -> Dict[str, Any]:
        """Optimize MongoDB performance"""
        try:
            logger.info("âš¡ Optimizing database performance")
            
            # Run database optimization
            result = await optimize_database_indexes()
            
            logger.info(f"âœ… Database optimization completed: {result.get('success', False)}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Error in database optimization: {e}")
            raise
    
    async def _generate_final_status(self, user_id: str) -> Dict[str, Any]:
        """Generate final processing status"""
        try:
            db = DatabaseService.get_database()
            
            # Get final counts
            total_emails = await db.email_logs.count_documents({"user_id": user_id})
            categorized_emails = await db.email_logs.count_documents({
                "user_id": user_id,
                "classification_status": {"$in": ["classified", "extracted"]}
            })
            financial_transactions = await db.financial_transactions.count_documents({"user_id": user_id})
            
            # Get category distribution
            pipeline = [
                {"$match": {"user_id": user_id}},
                {"$group": {"_id": "$email_category", "count": {"$sum": 1}}},
                {"$sort": {"count": -1}}
            ]
            category_distribution = await db.email_logs.aggregate(pipeline).to_list(length=None)
            
            # Get merchant distribution for financial transactions
            merchant_pipeline = [
                {"$match": {"user_id": user_id}},
                {"$group": {"_id": "$merchant_name", "count": {"$sum": 1}}},
                {"$sort": {"count": -1}},
                {"$limit": 10}
            ]
            merchant_distribution = await db.financial_transactions.aggregate(merchant_pipeline).to_list(length=None)
            
            return {
                "total_emails": total_emails,
                "categorized_emails": categorized_emails,
                "categorization_percentage": (categorized_emails / total_emails * 100) if total_emails > 0 else 0,
                "financial_transactions": financial_transactions,
                "category_distribution": category_distribution,
                "top_merchants": merchant_distribution,
                "status": "completed" if categorized_emails == total_emails else "in_progress"
            }
            
        except Exception as e:
            logger.error(f"âŒ Error generating final status: {e}")
            raise
    
    async def monitor_progress(self, user_id: str) -> Dict[str, Any]:
        """Monitor processing progress"""
        try:
            # Get categorization status
            categorization_status = await get_categorization_status(user_id)
            
            # Get financial extraction status
            financial_status = await get_financial_extraction_status(user_id)
            
            # Get database performance stats
            performance_stats = await get_query_performance_stats()
            
            return {
                "user_id": user_id,
                "categorization_status": categorization_status,
                "financial_status": financial_status,
                "performance_stats": performance_stats,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ Error monitoring progress: {e}")
            raise

async def main():
    """Main function to run scalable email processing"""
    try:
        # Use the correct user ID from the database
        user_id = "687a27f91a9e0ecec8a8c585"  # Correct user ID from database
        
        logger.info(f"ğŸš€ Starting scalable email processing")
        logger.info(f"ğŸ‘¤ User ID: {user_id}")
        
        # Initialize the scalable processor
        processor = ScalableEmailProcessor()
        
        # Process emails with scalable architecture
        result = await processor.process_user_emails_scalable(
            user_id=user_id,
            email_limit=None,  # Process all emails
            force_reprocess=False  # Don't reprocess already processed emails
        )
        
        if result["success"]:
            logger.info("=" * 60)
            logger.info("ğŸ“‹ FINAL RESULTS")
            logger.info("=" * 60)
            logger.info(f"âœ… Success: {result['success']}")
            logger.info(f"ğŸ‘¤ User ID: {result['user_id']}")
            logger.info(f"ğŸ“§ Total emails: {result['total_emails']}")
            logger.info(f"â±ï¸ Processing time: {result['processing_time']:.2f} seconds")
            
            # Show categorization results
            cat_result = result['categorization_result']
            logger.info(f"ğŸ“‚ Categorized emails: {cat_result.categorized_emails}")
            logger.info(f"ğŸ“Š Categories found: {len(cat_result.categories_found)}")
            
            # Show financial results
            fin_result = result['financial_result']
            logger.info(f"ğŸ’³ Financial transactions: {fin_result.get('extracted_transactions', 0)}")
            
            # Show final status
            final_status = result['final_status']
            logger.info(f"ğŸ“Š Categorization percentage: {final_status['categorization_percentage']:.1f}%")
            logger.info(f"ğŸ’³ Total financial transactions: {final_status['financial_transactions']}")
            
            # Show top categories
            logger.info(f"ğŸ·ï¸ Top categories:")
            for cat in final_status['category_distribution'][:5]:
                logger.info(f"   {cat['_id']}: {cat['count']}")
            
            # Show top merchants
            logger.info(f"ğŸ¢ Top merchants:")
            for merchant in final_status['top_merchants'][:5]:
                logger.info(f"   {merchant['_id']}: {merchant['count']}")
            
            logger.info("=" * 60)
            
        else:
            logger.error(f"âŒ Processing failed: {result.get('error', 'Unknown error')}")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ Error in main: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main()) 